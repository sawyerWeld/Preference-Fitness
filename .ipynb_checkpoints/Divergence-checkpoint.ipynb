{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Divergence of Empirical and Theoretical Probabilities of Rankings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler(KL) divergence of two probability distributions is a measure of difference between the two probability distributions. For probability distributions E and T, the KL divergence is \n",
    "\n",
    "$$ D_{KL}(P, Q) = \\sum_{i}Q(i)\\log\\frac{Q(i)}{P(i)}    $$\n",
    "\n",
    "where i is the ith term that the probability distribution is defined over. To find the KL divergence between the empirical and theoretical probability distributions of the Ireland 2002 data, we first load in the data as well as the parameters we found for the Mallows and Plackett-Luce models that best fit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([3, 4, 2, 1]), 0.02973511934833109],\n",
       " array([0.39292042, 0.08434978, 0.40365703, 0.11907276]),\n",
       " 475.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import readPreflib\n",
    "import numpy as np\n",
    "\n",
    "_, lengths, votes = readPreflib.soiInputwithWeights('data_input/ED-debian-2002.soi')\n",
    "num_votes = 1.0 * sum(lengths.values())\n",
    "\n",
    "import pickle\n",
    "\n",
    "mallows_params  = pickle.load( open('pickle/mallows2002_100k.p','rb') )\n",
    "sigma, phi = mallows_params\n",
    "plackett_params = pickle.load( open('pickle/plackett2002_100k.p','rb')) \n",
    "pl_weights = plackett_params\n",
    "\n",
    "mallows_params, plackett_params, num_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to gather the probability functions for the Mallows and Plackett-Luce models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Mallows_Notebook import mallowsProb\n",
    "from PL_Notebook import probPlackett\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can follow the equation for KL divergence to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351.02543014684534, 44.84313451440723)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divergence_mallows = 0\n",
    "divergence_plackett = 0\n",
    "for entry in votes:\n",
    "    num_occurances, vote = entry\n",
    "    empirical = num_occurances / num_votes\n",
    "    mallows = mallowsProb(vote, sigma, phi)\n",
    "    plackett = probPlackett(vote, pl_weights)\n",
    "    divergence_mallows += insideSum(mallows, empirical)\n",
    "    divergence_plackett += insideSum(plackett, empirical)\n",
    "    \n",
    "def insideSum(Qi,Pi):\n",
    "    return Qi * math.log(Qi/Pi)\n",
    "\n",
    "divergence_mallows, divergence_plackett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

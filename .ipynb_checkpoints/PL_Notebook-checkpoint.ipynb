{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plackett-Luce params for Debian 2002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a ranking {0,...,N} given weight vector W is\n",
    "\n",
    "$$ \\frac{W_0}{W_0+W_1+\\ldots+W_{N-1}}\\times\\frac{W_1}{W_1+W_2+\\ldots+W_{N-1}}\\times\\ldots\\times\\frac{W_{N-2}}{W_{N-2}+W_{N-1}}\\times \\frac{W_{N-1}}{W_{N-1}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import readPreflib\n",
    "import metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    }
   ],
   "source": [
    "def probPlackett(r, weights):\n",
    "    product = 1\n",
    "    for i in range(0,len(r)):\n",
    "        numer = getWeight(r[i],weights)\n",
    "        denom = 0\n",
    "        for j in range(i,len(r)):\n",
    "            denom += getWeight(r[j],weights)\n",
    "        if denom == 0:\n",
    "            product *= numer\n",
    "        else:\n",
    "            product *= (1.0 * numer) / denom\n",
    "    return product\n",
    "        \n",
    "def probPlackett2(r, weights):\n",
    "    print(r, weights)\n",
    "    return probPlackett(r, weights)\n",
    "    \n",
    "# alternatives are 1-indexed in the preflib data\n",
    "# kept forgetting this so made it a seperate method\n",
    "def getWeight(num, weights):\n",
    "    return weights[num-1]\n",
    "\n",
    "print(probPlackett(np.asarray([1, 2, 3, 4]),np.asarray([0.5,0.25,0.125,0.125])))\n",
    "# This should be 1/8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Branden Robinson',\n",
       " 2: 'Raphael Hertzog',\n",
       " 3: 'Bdale Garbee',\n",
       " 4: 'None Of The Above'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates, length_counts, votes = readPreflib.soiInputwithWeights('data_input/ED-debian-2002.soi')\n",
    "candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are votes in the data that are incomplete. We store a vector with the probabily of each length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLengthProbs(length_counts):\n",
    "    length_probs = []\n",
    "    total_votes = 1.0 * sum(length_counts.values())\n",
    "    for i in range(1,len(length_counts.values())+1):\n",
    "        length_probs.append(length_counts[i] / total_votes)\n",
    "    return length_probs\n",
    "    \n",
    "def probLength(lengths, n):\n",
    "    return lengths[n-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The votes come in as tuples that look like\n",
    "\n",
    "* (5, [1,2,3,4,5])\n",
    "* (2, [4,2,1,3])\n",
    "\n",
    "The second term in the tuple is a vote, and the first term is the number of terms that vote occurs. Therefore, the sum of the probabilities of all votes in a dataset given a plackett luce model is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plackettCost(params, dataset, lengths):\n",
    "    weights = params\n",
    "    cost = 0\n",
    "    for tup in dataset:\n",
    "        num_occurances, r = tup\n",
    "        cost += probLength(lengths, len(r)) * num_occurances * probPlackett(r, weights)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a set of weights to start the metropolis algorithm at. We can assign these randomly and then normalize as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomWeights(N):\n",
    "    weights = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        weights[i] = np.random.uniform()\n",
    "        s = np.sum(weights)\n",
    "        for i in range(N):\n",
    "            weights[i] = weights[i] / s\n",
    "    return weights\n",
    "\n",
    "def uniformWeights(N):\n",
    "    weights = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        weights[i] = 1.0 / N\n",
    "    return weights\n",
    "\n",
    "# random_weights = randomWeights(4)\n",
    "# print(random_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to move mass within the weights, which is how we generate a new candidate for the metropolis hastings algorithm. Here we transfer some mass from one alternative, j, to another, i. The limit on mass transfered = Δ'(Wᵢ→Wⱼ) = Argmin(Wᵢ,1-Wⱼ). The mass transfered = Δ = U(0,αΔ') where α is a parameter indicating the aggresiveness of the transfer.\n",
    "\n",
    "I think we could very easily decrease the aggresiveness over time, similar to how the 'temperature' in simulated annealing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferMass(weights, aggresiveness = 0.05):\n",
    "    w = list(weights)\n",
    "    N = len(w)\n",
    "    index1 = random.randint(0,N-1)\n",
    "    index2 = random.randint(0,N-1)\n",
    "    while (index2 == index1):\n",
    "        index2 = random.randint(0,N-1)\n",
    "\n",
    "    initial1 = w[index1]\n",
    "    initial2 = w[index2]\n",
    "    limit  = min(initial1, 1.0 - initial2)\n",
    "    delta = np.random.uniform(0.0, limit * aggresiveness)\n",
    "    w[index1] = initial1 - delta\n",
    "    w[index2] = initial2 + delta\n",
    "    return np.asarray(w)\n",
    "\n",
    "# print(transferMass(random_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to find parameters using the Metropolis Hastings algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPL(rankings, n_runs, lengths_vector):\n",
    "    lengths = getLengthProbs(lengths_vector)\n",
    "    initial_weights = uniformWeights(len(lengths))\n",
    "    params, cost = metropolis.maximize(plackettCost, initial_weights, lengths, transferMass, rankings, n_runs)\n",
    "    return params\n",
    "    \n",
    "#runPL(votes, 100, length_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(params, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell saves the model as a python pickle to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle.dump(params, open('pickle/plackett2002_3mil_2.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a more general way to run this externally we use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
